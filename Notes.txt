Notes
-----

The Dockerfile is used to build docker images. Most of the time we want to build our images from
prebuilt images on the docker hub. We specify the base image we want using the FROM keyword.

    FROM node:17

There are different commands we can run in the Dockerfile like.. 

    WORKDIR to set the working directory in the container. 
    
    RUN to execute commands at build time, ie when the image is being created.
    
    COPY to copy files from our machine to the container.

    CMD to run commands at run time. Commands that run on the container after it is built and is running.

Each command creates a layer of the docker image. When we build the image for the first time each
command is run and the results are cached.

    This means docker is very efficient when rebuilding images. If nothing has changed in a command
    the cached results are used instead of it being run again.

    When docker sees that a command line has changed and therefor a layer is changed all the lines 
    after it are rerun.

Once we have a Dockerfile we can build the image from the Dockerfile. If we are in the directory of the
docker file we can run...

    docker build .

To build the image giving it a name...

    docker build -t <name> .

When we run the above command we can see the output docker gives where we got through the Dockerfile step
by step and build each layer of the image.

We can see images we have built...

    docker image ls -a

We can remove docker images...

    docker image rm <image-id> <image-id>

Once we have built the docker image we can create a container from it, we can run it using...

    docker run -d --name <container-name> <image name or id>

        We run the docker image in detached mode and give it a name

We can remove our running docker container using the force flag...

    docker rm <container-name> -f

By default docker containers can talk to any outside points, on the internet or local network. However any
outside services cannot communicate with the docker container. This means that by default our host machine
cannot access anything in the docker container! 

We can modify the docker run command to allow networking. The port on the right is the container port we
are expecting traffic on. The number on the left is the port that traffic is coming in from from outside
the container. Traffic to port 3000 on the host machine is forwarded to port 3000 on the docker container.
Any valid ports can be used.

    docker run -p <host machine port>:<container port> -d --name <container-name> <image name or id>
    docker run -p 3000:3000 -d --name my-container my-image

We can login to a docker container using the following command...

    docker exec -it <container id or name> <program to run>
    
    docker exec -it d1c6865229b9 bash

        The above will execute a bash shell on the container id in interactive mode.

When we login to our docker container we see that by default it is copying all the files into the container
this is unnecessary and a bad practice. We can use a .dockerignore file to prevent this. This works in
the same way that a .gitignore file does.

    Note that when we set up volume bind mount below the .dockerignore file is not used since the volume
    setup is exposing the entire volume on the host to the docker container. When we have no bind mount
    volumes the .dockerignore is used.

When we make a change to the index.js file in our app notice how the change is not reflected when we check
the page. This is because the image we are running contains the old code and not the new updated code.

    When we stop and remove the container, rebuild the image and run it again as show above we can
    see the new changes to the index.js file.

    This is obviously not practical at all.

The answer to the above is using docker volumes which allow us to have persistent data. Docker has a specific
volume type called a bind mount which allows us to sync a folder on the host machine to a folder within the
docker container.

To add a volume we can add another flag to the docker run command...

    docker run -v <local dir>:<dir on container> -p 3000:3000 -d --name <container-name> <image name or id>

    docker run -v $(pwd):/app -p 3000:3000 -d --name my-container my-image

        We can get the current directory using $(pwd)

The volume bind mount is only for development, we wouldn't have this in production since the code is not
being changed.

Once the volume is set up we see that the volume changes are synced but we still see that changes we make to
the index.js file are still not being reflected. Why?

    This is not an issue with docker but one with node.js, every time changes are made we need to restart
    the node server for those changes to be reflected. This is very inefficient.

    Luckily there is something called nodemon, which is a utility that monitors for changes in the source code
    files and automatically restart the node server.

Now that we have our volumes set up we want to get to a point where we have all our node_modules within
the container and not on the host machine. If we try to remove the node_modules folder on the host now docker
will crash.

We can view the logs for a docker container...

    docker logs <container name or id>

And we see the error nodemon: not found. This is because we have set up volume bind mount and when our container
starts we are overwriting everything in the /app directory of the docker container with our host project directory
therefor erasing the node_modules folder.

The solution to this is to create another volume when we are running the container. This is a different type of
volume called an anonymous volume, -v /app/node_modules, basically all this is saying is that we want to override
the previous bind mount and not allow any modification of node_modules in the container...

    docker run -v $(pwd):/app -v /app/node_modules -p 3000:3000 -d --name my-container my-image

In the above when we set up the volume bind mount the host can read and write files and the container can do the
same. We may not want this, we can set the mode to read only which can be useful especially in production where
we don't want a docker container modifying any source code...

    docker run -v $(pwd):/app:ro ...

We can set environment variables in the Dockerfile using ENV, so when we create an ENV PORT 3000 we can pass in
a value when running the container. Here we are changing the container port to 4000...

    docker run -v $(pwd):/app -v /app/node_modules --env PORT=4000 -p 3000:4000 -d --name node-app node-app-image

    There are often many environment variables for a container, passing them in by the command line can be
    a burden. We can create a .env file where we can access them and then use the following command instead...

        docker run -v $(pwd):/app -v /app/node_modules --env-file ./.env -p 3000:4000 -d --name node-app node-app-image

When we remove containers the volumes that they create are still saved on disk. We can choose to remove the volumes
by adding another flag...

    docker rm <container name or id> -fv

The docker run commands can get very long and confusing especially since real applications have many docker containers.
The solution to this is using docker-compose to streamline creating our containers. In docker compose our containers
are called services. The -d flag runs it in detached mode.

    docker-compose up -d

To stop a container, the -v flag removes the volumes...

    docker-compose down -v

When we run docker-compose again we will notice that it does not rebuild the image. It checks the existing images to
see if there is one that matches the name it is expecting

    When we make a change to the Dockerfile and run docker-compose it is still not rebuilding the image!

    When we make changes to the Dockerfile we need to tell docker-compose to rebuild the image(s).

        docker-compose up -d --build

The commands that are run for development and deployment are often different. We can utilize docker compose to have
a different setup depending on which situation we are in.

    We can have separate dockerfiles and docker compose files for production and development or they can be
    condensed into a single file.

To split up the docker-compose files we will create one that will have the common settings named...

    docker-compose.yml

Then we create two more docker compose files, one for development specific settings and one for production.

    docker-compose-dev.yml
    docker-compose-prod.yml

    We add a node specific environment variable, NODE_ENV, where we can specify whether it's in development 
    or production.

In order to run our new docker-compose setup with multiple files...

    docker-compose -f <base-file> -f <specific-file> up -d
    docker-compose -f docker-compose.yml -f docker-compose-dev.yml up -d

In order to stop the containers...

    docker-compose -f docker-compose.yml -f docker-compose-dev.yml down -v

We can rebuild the image(s)...

    docker-compose -f docker-compose.yml -f docker-compose-prod.yml up -d --build

When toggling between production and development we often may want different dependencies for the application. We
can use embedded bash scripts within the Dockerfile to act upon passed in arguments

    ARG NODE_ENV

    RUN if [ "$NODE_ENV" = "development" ]; \
            then npm install; \
            else npm install --only=production; \
            fi

In the above we pass in a ARG and depending on if the value of it is 'production' or 'development' we changed
the npm install command.

We need to pass this value in the docker compose file...

    build:
      context: .
      args:
        NODE_ENV: development

Context refers to the folder location where the Dockerfile is, and then we pass in the ARGS.

So now when we want to start or stop in development mode, REMEMBER the -v flag removes the volume...

    docker-compose -f docker-compose.yml -f docker-compose-dev.yml up -d --build
    docker-compose -f docker-compose.yml -f docker-compose-dev.yml down -v

And when we want to start and stop in production, REMEMBER the -v flag removes the volume...

    docker-compose -f docker-compose.yml -f docker-compose-dev.yml up -d --build
    docker-compose -f docker-compose.yml -f docker-compose-dev.yml down -v

Many times we only need to the base image and no customization is necessary. In this case we don't need a
docker file and can just add the service in docker-compose...

    mongo: 
      image: mongo
      environment: 
        - MONGO_INITDB_ROOT_USERNAME=root
        - MONGO_INITDB_ROOT_PASSWORD=secret

    We can then open a shell in the database container and run... mongo -u root ... and then enter the
    password to log in.

    Some basic mongo commands...

        To create a db... use <database-name>

        To list dbs, only those with contents... show dbs

        To add an entry to a collection... 
        
            db.<collection>.insert({"name":"My Book"})

        To list all items in a collection...

            db.<collection>.find()

When using a database image without specifying a volume, when we bring down the container all the information
we stored is now gone.

    What kind of volume should we use for a database container?

        We could use a bind mount, that is def an options. But we don't necessarily need access to the
        database from the host machine.

        We have dealt with anonymous volumes before but the issue with these is that there is no way to
        identify the volume. When using the -v flag the volume is deleted...
        
            docker-compose down -v

        We can create a named volume in a docker-compose file, note we also have to declare it...

            services
             mongo: 
              image: mongo
              volumes:
               - mongo-db:/data/db
            
            volumes:
             mongo-db:

        And just like above when bringing the containers down with the -v flag the named volumes are deleted.

Lets install mongoose to our node container.

    npm install mongoose

When we are setting up mongoose we need to determine the ip address of the mongodb container. We can first get
the container id using... docker ps

    Once we have the container id we can run...

        docker inspect <container-id>

The above works but there is no guarantee that docker will assign the same ip to a container each time it is
created. Luckily docker has networks which allow containers to communicate. To view the networks...

    docker network ls

And to get information about a specific network...

    docker network inspect <network-name-or-id>

We can already see that docker-compose created a custom default network for our application. Custom networks
in docker have dns, so when we want to have containers communicate we can use the name of the container or
the name of the service when referring to another container in place of an IP address.

    We can confirm this by logging into a container and pinging another.
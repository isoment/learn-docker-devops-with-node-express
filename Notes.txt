Notes
-----

The Dockerfile is used to build docker images. Most of the time we want to build our images from
prebuilt images on the docker hub. We specify the base image we want using the FROM keyword.

    FROM node:17

There are different commands we can run in the Dockerfile like.. 

    WORKDIR to set the working directory in the container. 
    
    RUN to execute commands at build time, ie when the image is being created.
    
    COPY to copy files from our machine to the container.

    CMD to run commands at run time. Commands that run on the container after it is built and is running.

Each command creates a layer of the docker image. When we build the image for the first time each
command is run and the results are cached.

    This means docker is very efficient when rebuilding images. If nothing has changed in a command
    the cached results are used instead of it being run again.

    When docker sees that a command line has changed and therefor a layer is changed all the lines 
    after it are rerun.

Once we have a Dockerfile we can build the image from the Dockerfile. If we are in the directory of the
docker file we can run...

    docker build .

To build the image giving it a name...

    docker build -t <name> .

When we run the above command we can see the output docker gives where we got through the Dockerfile step
by step and build each layer of the image.

We can see images we have built...

    docker image ls -a

We can remove docker images...

    docker image rm <image-id> <image-id>

Once we have built the docker image we can create a container from it, we can run it using...

    docker run -d --name <container-name> <image name or id>

        We run the docker image in detached mode and give it a name

We can remove our running docker container using the force flag...

    docker rm <container-name> -f

By default docker containers can talk to any outside points, on the internet or local network. However any
outside services cannot communicate with the docker container. This means that by default our host machine
cannot access anything in the docker container! 

We can modify the docker run command to allow networking. The port on the right is the container port we
are expecting traffic on. The number on the left is the port that traffic is coming in from from outside
the container. Traffic to port 3000 on the host machine is forwarded to port 3000 on the docker container.
Any valid ports can be used.

    docker run -p <host machine port>:<container port> -d --name <container-name> <image name or id>
    docker run -p 3000:3000 -d --name my-container my-image

We can login to a docker container using the following command...

    docker exec -it <container id or name> <program to run>
    
    docker exec -it d1c6865229b9 bash

        The above will execute a bash shell on the container id in interactive mode.

When we login to our docker container we see that by default it is copying all the files into the container
this is unnecessary and a bad practice. We can use a .dockerignore file to prevent this. This works in
the same way that a .gitignore file does.

When we make a change to the index.js file in our app notice how the change is not reflected when we check
the page. This is because the image we are running contains the old code and not the new updated code.

    When we stop and remove the container, rebuild the image and run it again as show above we can
    see the new changes to the index.js file.

    This is obviously not practical at all.

The answer to the above is using docker volumes which allow us to have persistent data. Docker has a specific
volume type called a bind mount which allows us to sync a folder on the host machine to a folder within the
docker container.

To add a volume we can add another flag to the docker run command...

    docker run -v <local dir>:<dir on container> -p 3000:3000 -d --name <container-name> <image name or id>

    docker run -v $(pwd):/app -p 3000:3000 -d --name my-container my-image

        We can get the current directory using $(pwd)

The volume bind mount is only for development, we wouldn't have this in production since the code is not
being changed.

Once the volume is set up we see that the volume changes are synced but we still see that changes we make to
the index.js file are still not being reflected. Why?

    This is not an issue with docker but one with node.js, every time changes are made we need to restart
    the node server for those changes to be reflected. This is very inefficient.

    Luckily there is something called nodemon, which is a utility that monitors for changes in the source code
    files and automatically restart the node server.

Now that we have our volumes set up we want to get to a point where we have all our node_modules within
the container and not on the host machine. If we try to remove the node_modules folder on the host now docker
will crash.

We can view the logs for a docker container...

    docker logs <container name or id>

And we see the error nodemon: not found. This is because we have set up volume bind mount and when our container
starts we are overwriting everything in the /app directory of the docker container with our host project directory
therefor erasing the node_modules folder.

The solution to this is to create another volume when we are running the container. This is a different type of
volume called an anonymous volume, -v /app/node_modules, basically all this is saying is that we want to override
the previous bind mount and not allow any modification of node_modules in the container...

    docker run -v $(pwd):/app -v /app/node_modules -p 3000:3000 -d --name my-container my-image

In the above when we set up the volume bind mount the host can read and write files and the container can do the
same. We may not want this, we can set the mode to read only which can be useful especially in production where
we don't want a docker container modifying any source code...

    docker run -v $(pwd):/app:ro ...